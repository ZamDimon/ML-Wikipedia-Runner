{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "4476c25a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9223372036854775807"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "from importlib import reload\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import sys\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "#reload(sys)\n",
    "#sys.setdefaultencoding('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "636afb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    new_text = \"\"\n",
    "    prev = ' '\n",
    "    delete_1 = 0\n",
    "    delete_2 = 0\n",
    "    delete_3 = 0\n",
    "    for i in enumerate(text):\n",
    "        if (delete_1 <= 0 and delete_2 <= 0 and delete_3 <=0) and (i[1] != '{' and i[1] != '[' and i[1] != '<'):\n",
    "            new_text =  new_text + i[1]\n",
    "        if (i[1] == '[' and prev == '['):\n",
    "            delete_2 += 1\n",
    "        if (i[1] == '{' and prev == '{'):\n",
    "            delete_1 += 1\n",
    "        if i[1] == '}' and prev == '}':\n",
    "            delete_1 -= 1\n",
    "        if i[1] == ']' and prev == ']':\n",
    "            delete_2 -= 1\n",
    "            \n",
    "        if i[1] == '<':\n",
    "            delete_3 += 1\n",
    "        if i[1] == '>':\n",
    "            delete_3 -= 1\n",
    "        prev = i[1]\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "4151f00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "12793ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "## set directories and parameters\n",
    "########################################\n",
    "BASE_DIR = '~/projects/ML-Wikipedia-Runner/'\n",
    "EMBEDDING_FILE = BASE_DIR + 'internal/dataset_generator/output/GoogleNews-vectors-negative300.bin'\n",
    "TRAIN_DATA_FILE = '../dataset_generator/output/train.csv'\n",
    "TEST_DATA_FILE =  '../dataset_generator/output/test.csv'\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "MAX_NB_WORDS = 400000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "num_lstm = np.random.randint(175, 275)\n",
    "num_dense = np.random.randint(100, 150)\n",
    "rate_drop_lstm = 0.15 + np.random.rand() * 0.25\n",
    "rate_drop_dense = 0.15 + np.random.rand() * 0.25\n",
    "\n",
    "act = 'relu'\n",
    "re_weight = True # whether to re-weight classes to fit the 17.5% share in test set\n",
    "\n",
    "STAMP = 'lstm_%d_%d_%.2f_%.2f'%(num_lstm, num_dense, rate_drop_lstm, \\\n",
    "        rate_drop_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "0db85b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors\n",
      "Found 3000000 word vectors of word2vec\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## index word vectors\n",
    "########################################\n",
    "print('Indexing word vectors')\n",
    "\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, \\\n",
    "        binary=True)\n",
    "print('Found %s word vectors of word2vec' % len(word2vec.key_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "bc35882d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n",
      "Found 1004 texts in train.csv\n",
      "Found 201 texts in test.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "########################################\n",
    "## process texts in datasets\n",
    "########################################\n",
    "print('Processing text dataset')\n",
    "\n",
    "# The function \"text_to_wordlist\" is from\n",
    "# https://www.kaggle.com/currie32/quora-question-pairs/the-importance-of-cleaning-text\n",
    "\n",
    "texts_1 = [] \n",
    "texts_2 = []\n",
    "labels = []\n",
    "names_1 = []\n",
    "names_2 = []\n",
    "with codecs.open(TRAIN_DATA_FILE, encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    header = next(reader)\n",
    "    for values in reader:\n",
    "        names_1.append(values[0])\n",
    "        texts_1.append(text_to_wordlist(preprocess_text(values[1]))[:1000])\n",
    "        names_2.append(values[2])\n",
    "        texts_2.append(text_to_wordlist(preprocess_text(values[3]))[:1000])\n",
    "        labels.append(int(values[4]))\n",
    "print('Found %s texts in train.csv' % len(texts_1))\n",
    "test_texts_1 = []\n",
    "test_texts_2 = []\n",
    "test_ids = []\n",
    "with codecs.open(TEST_DATA_FILE, encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    header = next(reader)\n",
    "    for values in reader:\n",
    "        test_texts_1.append(text_to_wordlist(preprocess_text(values[1])))\n",
    "        test_texts_2.append(text_to_wordlist(preprocess_text(values[3])))\n",
    "        test_ids.append(values[4])\n",
    "print('Found %s texts in test.csv' % len(test_texts_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "30523fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 63442 unique tokens\n",
      "Shape of data tensor: (1004, 30)\n",
      "Shape of label tensor: (1004,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts_1 + texts_2 + test_texts_1 + test_texts_2)\n",
    "\n",
    "sequences_1 = tokenizer.texts_to_sequences(texts_1)\n",
    "sequences_2 = tokenizer.texts_to_sequences(texts_2)\n",
    "test_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)\n",
    "test_sequences_2 = tokenizer.texts_to_sequences(test_texts_2)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "data_1 = pad_sequences(sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data_2 = pad_sequences(sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = np.array(labels)\n",
    "print('Shape of data tensor:', data_1.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "test_data_1 = pad_sequences(test_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_data_2 = pad_sequences(test_sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_ids = np.array(test_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "744013df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1004, 30)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "dcc9ea5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: 34548\n"
     ]
    }
   ],
   "source": [
    "\n",
    "########################################\n",
    "## prepare embeddings\n",
    "########################################\n",
    "print('Preparing embedding matrix')\n",
    "\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))+1\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec.key_to_index:\n",
    "        embedding_matrix[i] = word2vec.get_vector(word)\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "85786f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(324555, 300)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "8b96b56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "########################################\n",
    "## sample train/validation data\n",
    "########################################\n",
    "#np.random.seed(1234)\n",
    "perm = np.random.permutation(len(data_1))\n",
    "idx_train = perm[:int(len(data_1)*(1-VALIDATION_SPLIT))]\n",
    "idx_val = perm[int(len(data_1)*(1-VALIDATION_SPLIT)):]\n",
    "\n",
    "data_1_train = np.hstack((data_1[idx_train], data_2[idx_train]))\n",
    "data_2_train = np.hstack((data_2[idx_train], data_1[idx_train]))\n",
    "labels_train = labels[idx_train]\n",
    "\n",
    "data_1_val = np.hstack((data_1[idx_val], data_2[idx_val]))\n",
    "data_2_val = np.hstack((data_2[idx_val], data_1[idx_val]))\n",
    "labels_val = labels[idx_val]\n",
    "\n",
    "weight_val = np.ones(len(labels_val))\n",
    "if re_weight:\n",
    "    weight_val *= 0.472001959\n",
    "    weight_val[labels_val==0] = 1.309028344\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "7a2f1386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6229954650820686 [0.7362768  0.55888144 0.61938811 0.57743551]\n"
     ]
    }
   ],
   "source": [
    "# Training our model\n",
    "params = {'learning_rate':0.01, 'n_estimators':600,\n",
    "                    'nthread': 1, 'subsample': 0.6, 'min_child_weight': 1, 'max_depth': 5, 'gamma': 1.5, 'colsample_bytree': 0.8}\n",
    "xgb_reg = XGBRegressor(**params)\n",
    "#start_time = timer(None)\n",
    "scores = cross_val_score(xgb_reg, data_1_train, labels_train, scoring=make_scorer(mean_squared_error),  cv=4)\n",
    "print(scores.mean(), scores)\n",
    "#timer(start_time) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "c1a53e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=0.8, enable_categorical=False,\n",
       "             gamma=1.5, gpu_id=-1, importance_type=None,\n",
       "             interaction_constraints='', learning_rate=0.01, max_delta_step=0,\n",
       "             max_depth=5, min_child_weight=1, missing=nan,\n",
       "             monotone_constraints='()', n_estimators=600, n_jobs=1, nthread=1,\n",
       "             num_parallel_tree=1, predictor='auto', random_state=0, reg_alpha=0,\n",
       "             reg_lambda=1, scale_pos_weight=1, subsample=0.6,\n",
       "             tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_reg.fit(data_1_train[:500], labels_train[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "b52a2856",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/utils/validation.py:63: FutureWarning: Arrays of bytes/strings is being converted to decimal numbers if dtype='numeric'. This behavior is deprecated in 0.24 and will be removed in 1.1 (renaming of 0.26). Please convert your data to numeric values explicitly instead.\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.07297732980881"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(xgb_reg.predict(np.hstack((test_data_1, test_data_2))), test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "6fed78b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4669602050665907"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(xgb_reg.predict(data_1_train[600:700]), labels_train[600:700])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "c339e8ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.1303465, 3.0433836, 3.136864 , 2.688777 , 2.9511845, 3.0433836,\n",
       "       3.1653183, 2.5575051, 2.7755616, 2.7755616, 2.9840698, 2.2381618,\n",
       "       3.0725186, 2.6757746, 2.7151797, 2.7314122, 2.6757746, 2.7151797,\n",
       "       2.6008651, 2.6166809, 2.6757746, 2.259337 , 2.0247102, 2.6867118,\n",
       "       2.5714781, 3.0881166, 2.6867118, 3.130042 , 2.7138758, 2.7257798,\n",
       "       3.3675303, 3.050821 , 3.4508774, 3.2472515, 2.8035843, 2.6080627,\n",
       "       2.860757 , 2.6160955, 3.3675303, 2.8035843, 2.6080627, 2.6080627,\n",
       "       2.1389406, 2.6442332, 2.7106433, 2.8237283, 2.7888672, 2.6442332,\n",
       "       2.6442332, 2.8445463, 3.2973542, 3.2265408, 2.8172848, 2.6606052,\n",
       "       2.686128 , 2.831532 , 3.5706036, 3.120156 , 3.1991723, 2.771214 ,\n",
       "       2.5169132, 2.8323712, 3.1766186, 3.5706036, 3.120156 , 2.5169132,\n",
       "       2.5169132, 2.520507 , 2.7403638, 2.4195626, 2.6093159, 2.520507 ,\n",
       "       2.7403638, 2.451008 , 2.520507 , 2.6591842, 3.0316966, 2.4128747,\n",
       "       2.581084 , 1.9490469, 2.6683645, 2.783653 , 3.1003287, 2.6050506,\n",
       "       3.32821  , 3.1003287, 2.6050506, 3.2463937, 3.1003287, 2.5750766,\n",
       "       1.9699845, 2.4979157, 2.8549829, 2.2088375, 2.596837 , 2.411529 ,\n",
       "       2.4336507, 2.5355995, 2.6992033, 2.6603456, 2.7699854, 2.411529 ,\n",
       "       2.4064791, 2.6992033, 2.6223044, 2.9132724, 2.9658935, 2.949715 ,\n",
       "       2.831015 , 2.8459659, 2.454514 , 3.227158 , 2.8791614, 3.0103116,\n",
       "       2.7255268, 2.8791614, 3.0686524, 2.4475017, 3.2016356, 3.0964236,\n",
       "       2.774933 , 3.2016356, 3.0964236, 3.0550015, 3.2016356, 3.1727424,\n",
       "       2.5775003, 2.8232348, 2.8329375, 3.3436987, 2.8507924, 3.1387103,\n",
       "       3.2270777, 2.4562712, 2.8439956, 3.1727424, 2.745561 , 3.0801237,\n",
       "       2.586622 , 2.906679 , 3.008693 , 2.8954115, 2.6742635, 2.8374124,\n",
       "       2.7981524, 2.609989 , 2.5779014, 2.6871052, 2.7968464, 2.972221 ,\n",
       "       2.6742635, 2.525674 , 2.6058745, 2.9873652, 2.8597672, 2.997982 ,\n",
       "       2.6002178, 2.684262 , 2.6871052, 2.6871052, 2.3064964, 2.4876037,\n",
       "       2.718511 , 2.8597672, 3.25824  , 2.897489 , 3.0255342, 3.25824  ,\n",
       "       2.2429397, 2.7015467, 3.1891882, 2.2526903, 2.6382194, 2.7527342,\n",
       "       2.9810133, 2.625494 , 2.4399662, 2.2883685, 2.385587 , 1.7310191,\n",
       "       2.7196531, 2.7545753, 2.7088077, 3.1610475, 2.2883685, 2.385587 ,\n",
       "       2.7545753, 2.670416 , 3.1088498, 2.8459737, 3.058987 , 3.058987 ,\n",
       "       2.888952 , 3.0375013, 2.9863515, 2.7504046, 2.6929803, 2.8933113,\n",
       "       2.7504046, 2.6929803, 2.7056856], dtype=float32)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_reg.predict(np.hstack((test_data_1, test_data_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "f180cafb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2', '4', '3', '3', '1', '4', '3', '2', '3', '3', '2', '2', '1',\n",
       "       '4', '3', '3', '4', '3', '3', '1', '4', '2', '2', '2', '2', '2',\n",
       "       '2', '2', '2', '2', '5', '4', '3', '2', '4', '3', '3', '2', '5',\n",
       "       '4', '3', '3', '1', '3', '2', '1', '2', '3', '3', '2', '4', '3',\n",
       "       '3', '4', '3', '3', '5', '4', '3', '4', '3', '3', '2', '5', '4',\n",
       "       '3', '3', '4', '3', '3', '1', '4', '3', '3', '4', '2', '1', '1',\n",
       "       '4', '3', '3', '3', '4', '3', '3', '4', '3', '3', '4', '1', '2',\n",
       "       '2', '1', '2', '2', '5', '4', '3', '4', '3', '3', '5', '4', '4',\n",
       "       '3', '2', '3', '2', '3', '3', '3', '3', '3', '2', '1', '3', '1',\n",
       "       '3', '4', '3', '3', '4', '3', '3', '4', '6', '5', '4', '3', '5',\n",
       "       '4', '3', '4', '3', '3', '6', '4', '3', '3', '2', '3', '2', '5',\n",
       "       '4', '3', '2', '4', '3', '3', '1', '5', '4', '3', '3', '2', '3',\n",
       "       '3', '3', '3', '3', '2', '2', '2', '2', '3', '3', '2', '3', '2',\n",
       "       '3', '3', '2', '1', '1', '2', '1', '2', '5', '4', '3', '4', '3',\n",
       "       '3', '2', '5', '4', '3', '2', '3', '1', '3', '3', '2', '2', '2',\n",
       "       '4', '3', '3', '4', '3', '2'], dtype='<U1')"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "f321f62a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 4, 3, 4, 4, 3, 3, 2, 3, 3, 2, 4, 4, 3, 3, 2, 5, 2, 3, 4, 2,\n",
       "       3, 4, 6, 3, 3, 3, 3, 3, 3, 4, 3, 1, 2, 1, 3, 4, 3, 3, 2, 3, 3, 3,\n",
       "       2, 2, 2, 4, 4, 3, 2, 4, 3, 3, 2, 4, 1, 4, 4, 1, 2, 4, 4, 3, 3, 3,\n",
       "       1, 2, 3, 3, 3, 3, 3, 3, 5, 2, 5, 4, 3, 3, 3, 3, 3, 3, 3, 2, 3, 5,\n",
       "       3, 3, 3, 3, 3, 4, 2, 2, 4, 5, 3, 3, 2])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "5cb6699d",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "## define the model structure\n",
    "########################################\n",
    "embedding_layer = Embedding(nb_words,\n",
    "        EMBEDDING_DIM,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        trainable=False)\n",
    "lstm_layer = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)\n",
    "\n",
    "sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH*2,), dtype='int32')\n",
    "embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH*2,), dtype='int32')\n",
    "embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "y1 = lstm_layer(embedded_sequences_2)\n",
    "\n",
    "merged = concatenate([x1, y1])\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "merged = Dense(num_dense, activation=act)(merged)\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "#preds = Dense(1, activation='sigmoid')(merged)\n",
    "preds = Dense(1, activation='ReLU')(merged)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "7d93641d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, 3, 2, 3, 2, 2, 3, 3, 4, 3, 1, 5, 4, 2, 3, 5, 3, 4, 3, 3,\n",
       "       3, 3, 4, 2, 3, 1, 5, 1, 3, 1, 2, 2, 4, 3, 3, 1, 3, 3, 4, 2, 2, 3,\n",
       "       2, 4, 2, 6, 4, 2, 1, 3, 3, 2, 5, 4, 2, 3, 3, 5, 3, 3, 2, 5, 3, 1,\n",
       "       3, 5, 3, 2, 3, 5, 2, 2, 4, 1, 3, 2, 2, 3, 1, 2, 4, 5, 2, 3, 3, 3,\n",
       "       2, 3, 4, 3, 4, 1, 4, 3, 4, 3, 4, 2, 4, 3, 3, 3, 3, 2, 3, 2, 2, 3,\n",
       "       3, 4, 3, 1, 5, 4, 2, 3, 5, 3, 4, 3, 3, 3, 3, 4, 2, 3, 1, 5, 1, 3,\n",
       "       1, 2, 2, 4, 3, 3, 1, 3, 3, 4, 2, 2, 3, 2, 4, 2, 6, 4, 2, 1, 3, 3,\n",
       "       2, 5, 4, 2, 3, 3, 5, 3, 3, 2, 5, 3, 1, 3, 5, 3, 2, 3, 5, 2, 2, 4,\n",
       "       1, 3, 2, 2, 3, 1, 2, 4, 5, 2, 3, 3, 3, 2, 3, 4, 3, 4, 1, 4, 3, 4,\n",
       "       3, 4, 2, 4])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "515eb85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm_179_135_0.35_0.27\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 16s 16s/step - loss: 7.2814 - mse: 7.2814 - val_loss: 9.6615 - val_mse: 9.6615\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 4s 4s/step - loss: 6.9390 - mse: 6.9390 - val_loss: 9.5284 - val_mse: 9.5284\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 4s 4s/step - loss: 6.6207 - mse: 6.6207 - val_loss: 9.5427 - val_mse: 9.5427\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 4s 4s/step - loss: 6.1835 - mse: 6.1835 - val_loss: 9.5460 - val_mse: 9.5460\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 4s 4s/step - loss: 6.0018 - mse: 6.0018 - val_loss: 9.5822 - val_mse: 9.5822\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "########################################\n",
    "## train the model\n",
    "########################################\n",
    "model = Model(inputs=[sequence_1_input, sequence_2_input], \\\n",
    "        outputs=preds)\n",
    "model.compile(loss='mse',\n",
    "        optimizer='nadam',\n",
    "        metrics=['mse'])\n",
    "#model.summary()\n",
    "print(STAMP)\n",
    "\n",
    "early_stopping =EarlyStopping(monitor='val_mse', patience=3)\n",
    "bst_model_path = STAMP + '.h5'\n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "hist = model.fit([data_1_train, data_2_train], labels_train, \\\n",
    "        validation_data=([data_1_val, data_2_val], labels_val), \\\n",
    "        epochs=200, batch_size=2048, shuffle=True, \\\n",
    "        callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "037aaefa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 1,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 1, 3, 3, 3, 1, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       2, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 1, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 1, 3,\n",
       "       3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 1, 3, 3, 3,\n",
       "       3, 3, 3, 3])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480d0cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "########################################\n",
    "## train the model\n",
    "########################################\n",
    "model = Model(inputs=[sequence_1_input, sequence_2_input], \\\n",
    "        outputs=preds)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "        optimizer='nadam',\n",
    "        metrics=['acc'])\n",
    "#model.summary()\n",
    "print(STAMP)\n",
    "\n",
    "early_stopping =EarlyStopping(monitor='val_loss', patience=3)\n",
    "bst_model_path = STAMP + '.h5'\n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "hist = model.fit([data_1_train, data_2_train], labels_train, \\\n",
    "        validation_data=([data_1_val, data_2_val], labels_val, weight_val), \\\n",
    "        epochs=200, batch_size=2048, shuffle=True, \\\n",
    "        class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "model.load_weights(bst_model_path)\n",
    "bst_val_score = min(hist.history['val_loss'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c30a22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "c56b5404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start making the submission before fine-tuning\n",
      "1/1 [==============================] - 0s 258ms/step\n",
      "1/1 [==============================] - 0s 226ms/step\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## make the submission\n",
    "########################################\n",
    "print('Start making the submission before fine-tuning')\n",
    "\n",
    "preds = model.predict([test_data_1, test_data_2], batch_size=2048, verbose=1)\n",
    "preds += model.predict([test_data_2, test_data_1], batch_size=2048, verbose=1)\n",
    "preds /= 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "384d4b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(201, 8)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "34019917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(201,)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "a0473dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.6496637, 2.769687 , 2.889439 , ..., 2.766019 , 2.6478813,\n",
       "        2.1968107],\n",
       "       [2.9501204, 3.0554934, 3.156005 , ..., 2.9867256, 2.8402689,\n",
       "        2.4281836],\n",
       "       [2.530469 , 2.6903658, 2.7935371, ..., 2.591871 , 2.4303348,\n",
       "        2.0488334],\n",
       "       ...,\n",
       "       [3.5676847, 3.678516 , 3.6293206, ..., 3.559912 , 3.4132476,\n",
       "        3.023573 ],\n",
       "       [3.259972 , 3.3953993, 3.2830296, ..., 3.1620235, 2.98708  ,\n",
       "        2.686414 ],\n",
       "       [3.0544474, 3.1235294, 3.1655955, ..., 2.9627635, 2.891192 ,\n",
       "        2.456123 ]], dtype=float32)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "9d72c856",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict([test_data_1, test_data_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "651117b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(201, 8)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "601467fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KerasTensor' object has no attribute 'min'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/0f/l86kbypj4714dmh9wtnc8lkr0000gn/T/ipykernel_22774/241673879.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mpreds\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m: 'KerasTensor' object has no attribute 'min'"
     ]
    }
   ],
   "source": [
    "preds.min(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f85ad79",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75f015e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}